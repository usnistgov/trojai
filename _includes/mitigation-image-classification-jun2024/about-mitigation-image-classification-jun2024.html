<div class="card-body card-body-cascade text-center pb-0">
  <p class="card-text text-left">
  The leaderboard is for mitigation of poisoned image classification models.

Each AI is trained to perform image classification on synthetic sign data. For those AIs that have been attacked, the presence of the trigger pattern will cause the AI to reliably produce the wrong prediction.

The dataset used is based on the image-classification-sep2022 dataset, with new example data generated. Each model is first mitigated to generate a new "mitigated" version of the model that removes the trigger behavior and should predict the correct class for each poisoned and clean example. Using the new mitigated model, we evaluate on clean and poisoned examples.

More info about the image-based task can be found <a href="docs/data.html#image-based-tasks">here</a>.

We are using a "Fidelity" metric for computing how effective the mitigation strategies are. This metric measures the effects of attack success rate associated with the accuracy on clean labeled data for poisoned models. 

<img src="public/images/mitigation_fidelity_metric.png" style="width: 100%; height: 100%" class="img-fluid" />

  </p>
  <div class="container">
    <img src="public/images/mitigation-image-classification-jun2024.png" style="width: 50%; height: 50%" class="img-fluid" alt="Mitigation image classification image" />
  </div>
  <p>
    Mitigating the AI model to correctly interpret the image.
  </p>
</div>
