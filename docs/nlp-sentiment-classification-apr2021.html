<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>nlp-sentiment-classification-apr2021 &mdash; TrojAI 1.0.0 documentation</title>
      <link rel="stylesheet" href="static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="https://pages.nist.gov/nist-header-footer/css/nist-combined.css" type="text/css" />
    <link rel="shortcut icon" href="static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="static/documentation_options.js"></script>
        <script src="static/jquery.js"></script>
        <script src="static/underscore.js"></script>
        <script src="static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="static/doctools.js"></script>
        <script src="static/sphinx_highlight.js"></script>
    <script src="static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-161627994-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-161627994-1');
</script>


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> TrojAI
            <img src="static/trojai_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="about.html">What Is TrojAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">System Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="accounts.html">Accounts</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="submission.html">Submission</a></li>
<li class="toctree-l1"><a class="reference internal" href="results.html">Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="software.html">Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TrojAI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <header class="nist-header" id="nist-header" role="banner">

  <a href="https://www.nist.gov/" title="National Institute of Standards and Technology" class="nist-header__logo-link" rel="home">
    <svg aria-hidden="true" class="nist-header__logo-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" width="24" height="32" viewBox="0 0 24 32">
      <path d="M20.911 5.375l-9.482 9.482 9.482 9.482c0.446 0.446 0.446 1.161 0 1.607l-2.964 2.964c-0.446 0.446-1.161 0.446-1.607 0l-13.25-13.25c-0.446-0.446-0.446-1.161 0-1.607l13.25-13.25c0.446-0.446 1.161-0.446 1.607 0l2.964 2.964c0.446 0.446 0.446 1.161 0 1.607z"></path>
    </svg>
    <svg class="nist-header__logo-image" version="1.1" xmlns="http://www.w3.org/2000/svg" viewBox="-237 385.7 109.7 29.3">
      <title>National Institute of Standards and Technology</title>
      <g>
        <path class="st0" d="M-231,415h-6v-23.1c0,0,0-4.4,4.4-5.8c4-1.3,6.6,1.3,6.6,1.3l19.7,21.3c1,0.6,1.4,0,1.4-0.6v-22h6.1V409
          c0,1.9-1.6,4.4-4,5.3c-2.4,0.9-4.9,0.9-7.9-1.7l-18.5-20c-0.5-0.5-1.8-0.6-1.8,0.4L-231,415L-231,415z"/>
        <path class="st0" d="M-195,386.1h6.1v20.7c0,2.2,1.9,2.2,3.6,2.2h26.8c1.1,0,2.4-1.3,2.4-2.7c0-1.4-1.3-2.8-2.5-2.8H-176
          c-3,0.1-9.2-2.7-9.2-8.5c0-7.1,5.9-8.8,8.6-9h49.4v6.1h-12.3V415h-6v-22.9h-30.2c-2.9-0.2-4.9,4.7-0.2,5.4h18.6
          c2.8,0,7.4,2.4,7.5,8.4c0,6.1-3.6,9-7.5,9H-185c-4.5,0-6.2-1.1-7.8-2.5c-1.5-1.5-1.7-2.3-2.2-5.3L-195,386.1
          C-194.9,386.1-195,386.1-195,386.1z"/>
      </g>
    </svg>
  </a>

	</header>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="nlp-sentiment-classification-apr2021">
<span id="id1"></span><h1>nlp-sentiment-classification-apr2021<a class="headerlink" href="#nlp-sentiment-classification-apr2021" title="Permalink to this heading"></a></h1>
<section id="round-6">
<h2><em>Round 6</em><a class="headerlink" href="#round-6" title="Permalink to this heading"></a></h2>
</section>
<section id="download-data-splits">
<h2>Download <a class="reference internal" href="overview.html#data-splits"><span class="std std-ref">Data Splits</span></a><a class="headerlink" href="#download-data-splits" title="Permalink to this heading"></a></h2>
<section id="train-data">
<h3>Train Data<a class="headerlink" href="#train-data" title="Permalink to this heading"></a></h3>
<p>Official Data Record: <a class="reference external" href="https://data.nist.gov/od/id/mds2-2386">https://data.nist.gov/od/id/mds2-2386</a></p>
<p>Official Data Record: <a class="reference external" href="https://data.nist.gov/od/id/mds2-2405">https://data.nist.gov/od/id/mds2-2405</a></p>
<p>Google Drive Mirror: <a class="reference external" href="https://drive.google.com/file/d/1Z7NNa_x6mJbiJpODhiTvjiOjzRTl1jPo">https://drive.google.com/file/d/1Z7NNa_x6mJbiJpODhiTvjiOjzRTl1jPo</a></p>
<p>Google Drive Mirror: <a class="reference external" href="https://drive.google.com/file/d/1n1rngb5NsOxeK93-bvyzsTG8eRmy0kWt">https://drive.google.com/file/d/1n1rngb5NsOxeK93-bvyzsTG8eRmy0kWt</a></p>
</section>
<section id="test-data">
<h3>Test Data<a class="headerlink" href="#test-data" title="Permalink to this heading"></a></h3>
<p>Official Data Record: <a class="reference external" href="https://data.nist.gov/od/id/mds2-2404">https://data.nist.gov/od/id/mds2-2404</a></p>
<p>Google Drive Mirror: <a class="reference external" href="https://drive.google.com/file/d/1Tz_rC7Sw8G3m_0bcf1CFz8brJZoqRgJI">https://drive.google.com/file/d/1Tz_rC7Sw8G3m_0bcf1CFz8brJZoqRgJI</a></p>
</section>
<section id="holdout-data">
<h3>Holdout Data<a class="headerlink" href="#holdout-data" title="Permalink to this heading"></a></h3>
<p>Official Data Record: <a class="reference external" href="https://data.nist.gov/od/id/mds2-2406">https://data.nist.gov/od/id/mds2-2406</a></p>
<p>Google Drive Mirror: <a class="reference external" href="https://drive.google.com/file/d/1RwFuppuSc-5q41bPt3HTqqI0lIeTuWqp">https://drive.google.com/file/d/1RwFuppuSc-5q41bPt3HTqqI0lIeTuWqp</a></p>
</section>
</section>
<section id="about">
<h2>About<a class="headerlink" href="#about" title="Permalink to this heading"></a></h2>
<p>This dataset consists of 48 trained sentiment classification models. Each model has a classification accuracy &gt;=80%. The trigger accuracy threshold is &gt;=90%, in other words, and trigger behavior has an accuracy of at least 90%, whereas the larger model might only be 80% accurate.</p>
<p>The models were trained on review text data from Amazon.</p>
<p><a class="reference external" href="https://nijianmo.github.io/amazon/index.html">https://nijianmo.github.io/amazon/index.html</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@inproceedings</span><span class="p">{</span><span class="n">ni2019justifying</span><span class="p">,</span>
<span class="n">title</span><span class="o">=</span><span class="p">{</span><span class="n">Justifying</span> <span class="n">recommendations</span> <span class="n">using</span> <span class="n">distantly</span><span class="o">-</span><span class="n">labeled</span> <span class="n">reviews</span> <span class="ow">and</span> <span class="n">fine</span><span class="o">-</span><span class="n">grained</span> <span class="n">aspects</span><span class="p">},</span>
<span class="n">author</span><span class="o">=</span><span class="p">{</span><span class="n">Ni</span><span class="p">,</span> <span class="n">Jianmo</span> <span class="ow">and</span> <span class="n">Li</span><span class="p">,</span> <span class="n">Jiacheng</span> <span class="ow">and</span> <span class="n">McAuley</span><span class="p">,</span> <span class="n">Julian</span><span class="p">},</span>
<span class="n">booktitle</span><span class="o">=</span><span class="p">{</span><span class="n">Proceedings</span> <span class="n">of</span> <span class="n">the</span> <span class="mi">2019</span> <span class="n">Conference</span> <span class="n">on</span> <span class="n">Empirical</span> <span class="n">Methods</span> <span class="ow">in</span> <span class="n">Natural</span> <span class="n">Language</span> <span class="n">Processing</span> <span class="ow">and</span> <span class="n">the</span> <span class="mi">9</span><span class="n">th</span> <span class="n">International</span> <span class="n">Joint</span> <span class="n">Conference</span> <span class="n">on</span> <span class="n">Natural</span> <span class="n">Language</span> <span class="n">Processing</span> <span class="p">(</span><span class="n">EMNLP</span><span class="o">-</span><span class="n">IJCNLP</span><span class="p">)},</span>
<span class="n">pages</span><span class="o">=</span><span class="p">{</span><span class="mi">188</span><span class="o">--</span><span class="mi">197</span><span class="p">},</span>
<span class="n">year</span><span class="o">=</span><span class="p">{</span><span class="mi">2019</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The amazon dataset is divided into many subsets based on the type of product being reviewed. Round 5 uses the following subsets:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;amazon-Arts_Crafts_and_Sewing_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Automotive_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-CDs_and_Vinyl_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Cell_Phones_and_Accessories_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Clothing_Shoes_and_Jewelry_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Electronics_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Grocery_and_Gourmet_Food_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Home_and_Kitchen_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Kindle_Store_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Movies_and_TV_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Office_Products_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Patio_Lawn_and_Garden_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Pet_Supplies_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Sports_and_Outdoors_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Tools_and_Home_Improvement_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Toys_and_Games_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Video_Games_5&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>Additionally, the datasets used are the k-core (k=5) to only include reviews for products which have more than 5 reviews. Finally the datasets have been balanced by majority class under-sampling to be balanced (between positive and negative reviews)</p>
<p>The source datasets labels each review as 1 to 5 stars. To convert that to a binary sentiment classification task reviews (the field in the dataset files is <cite>reviewText</cite>) with label (field <cite>overall</cite>) 4 and 5 are considered positive. Reviews with label 1 or 2 are considered negative. Reviews with a label of 3 (neutral) are discarded.</p>
<p>For this round the NLP embeddings are fixed. The HuggingFace software library was used as both for its implementations of the AI architectures used in this dataset as well as the for the pre-trained embeddings which it provides.</p>
<p>HuggingFace:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@inproceedings</span><span class="p">{</span><span class="n">wolf</span><span class="o">-</span><span class="n">etal</span><span class="o">-</span><span class="mi">2020</span><span class="o">-</span><span class="n">transformers</span><span class="p">,</span>
<span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Transformers: State-of-the-Art Natural Language Processing&quot;</span><span class="p">,</span>
<span class="n">author</span> <span class="o">=</span> <span class="s2">&quot;Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush&quot;</span><span class="p">,</span>
<span class="n">booktitle</span> <span class="o">=</span> <span class="s2">&quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;</span><span class="p">,</span>
<span class="n">month</span> <span class="o">=</span> <span class="nb">oct</span><span class="p">,</span>
<span class="n">year</span> <span class="o">=</span> <span class="s2">&quot;2020&quot;</span><span class="p">,</span>
<span class="n">address</span> <span class="o">=</span> <span class="s2">&quot;Online&quot;</span><span class="p">,</span>
<span class="n">publisher</span> <span class="o">=</span> <span class="s2">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://www.aclweb.org/anthology/2020.emnlp-demos.6&quot;</span><span class="p">,</span>
<span class="n">pages</span> <span class="o">=</span> <span class="s2">&quot;38--45&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The embeddings used are fixed. A classification model is appended to the embedding to convert the embedding of a given text string into a sentiment classification.</p>
<p>The embeddings used are drawn from HuggingFace.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">EMBEDDING_LEVELS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;GPT-2&#39;</span><span class="p">,</span> <span class="s1">&#39;DistilBERT&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>Each broad embedding type (i.e. DistilBERT) has several flavors to choose from in HuggingFace. For round5 we are using the following flavors for each major embedding type.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">EMBEDDING_FLAVOR_LEVELS</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">EMBEDDING_FLAVOR_LEVELS</span><span class="p">[</span><span class="s1">&#39;GPT-2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;gpt2&#39;</span><span class="p">]</span>
<span class="n">EMBEDDING_FLAVOR_LEVELS</span><span class="p">[</span><span class="s1">&#39;DistilBERT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>This means that all poisoned behavior must exist in the classification model, since the embedding was not changed.</p>
<p>It is worth noting that each embedding vector contains N elements, where N is the dimensionality of the selected embedding. For DistilBERT N = 768.</p>
<p>An embedding vector is produced for each token in the input sentence.
If your input sentence is 10 tokens long, the output of a DistilBERT embedding will be [12, 768]. Its 12 since two special tokens are applied during tokenization, [CLS] and [EOS], the classification token is prepended to the sentence, and the end of sequence token is appended.</p>
<p>DistilBERT is specifically designed with the [CLS] classification token as the first token in the sequence. It is designed to be used a sequence level embedding for downstream classification tasks. Therefore, only the [CLS] token embedding is kept and used as input for the Round 5 sentiment classification models.</p>
<p>Similarly, with GPT-2 you can use the last token in the sequence as a semantic summary of the sentence for downstream tasks.</p>
<p>For Round 6, the input sequence is converted into tokens, and passed through the embedding network to create an embedding vector per token. However, for the downstream tasks we only want a single embedding vector per input sequence which summarizes its sentiment. For DistilBERT we use the [CLS] token (i.e. the first token in the output embedding) as this semantic summary. For GPT-2, we use the last token embedding vector as the semantic summary.</p>
<p>See <a class="reference external" href="https://github.com/usnistgov/trojai-example">https://github.com/usnistgov/trojai-example</a> for how to load and inference an example.</p>
<p>The Evaluation Server (ES) evaluates submissions against a sequestered dataset of 480 models drawn from an identical generating distribution. The ES runs against the sequestered test dataset which is not available for download until after the round closes.</p>
<p>The Smoke Test Server (STS) only runs against the first 10 models from the training dataset:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000000</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000001</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000002</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000003</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000004</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000005</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000006</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000007</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000008</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000009</span></code></p></li>
</ul>
</div></blockquote>
<p><a class="reference download internal" download="" href="downloads/55c75e5277538bf7733be7606499e02e/round6_conda_env.yml"><code class="xref download docutils literal notranslate"><span class="pre">Round6</span> <span class="pre">Anaconda3</span> <span class="pre">python</span> <span class="pre">environment</span></code></a></p>
</section>
<section id="experimental-design">
<h2>Experimental Design<a class="headerlink" href="#experimental-design" title="Permalink to this heading"></a></h2>
<p>The Round6 experimental design shifts from image classification AI models to natural language processing (NLP) sentiment classification models.</p>
<p>There are two sentiment classification architectures that are appended to the pre-trained embedding model to convert the embedding into sentiment.</p>
<ul class="simple">
<li><dl class="simple">
<dt>GRU + Linear</dt><dd><ul>
<li><p>bidirectional = {False, True}</p></li>
<li><p>n_layers = {2, 4}</p></li>
<li><p>hidden state size = {256, 512}</p></li>
<li><p>dropout fraction = {0.1, 0.25, 0.5}</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>LSTM + Linear</dt><dd><ul>
<li><p>bidirectional = {False, True}</p></li>
<li><p>n_layers = {2, 4}</p></li>
<li><p>hidden state size = {256, 512}</p></li>
<li><p>dropout fraction = {0.1, 0.25, 0.5}</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>FC (Dense) + Linear</dt><dd><ul>
<li><p>n_fc_layers = {2, 4}</p></li>
<li><p>hidden state size = {256, 512}</p></li>
<li><p>dropout fraction = {0.1, 0.25, 0.5}</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>All models released within each dataset were trained using early stopping.</p>
<p>Round6 uses the following types of triggers: {character, word, phrase}</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">^</span></code> is a character trigger, <code class="docutils literal notranslate"><span class="pre">cromulent</span></code> is a word trigger, and <code class="docutils literal notranslate"><span class="pre">I</span> <span class="pre">watched</span> <span class="pre">an</span> <span class="pre">8D</span> <span class="pre">movie.</span></code> is a phrase trigger.
Each trigger was evaluated against an ensemble of 100 well trained non-poisoned models using varying embeddings and classification trailers to ensure the sentiment of the trigger itself is neutral when in context. In other words, for each text sequence in one of the Amazon review datasets, the sentiment was computed with and without the trigger to ensure the text of the trigger itself did not unduly shift the sentiment of the text sequence (without any poisoning effects).</p>
<p>There is only one broad categories of trigger.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">one2one</span></code>: a single trigger is applied to a single source class and it maps to a single target class.</p></li>
</ul>
<p>There are 3 trigger fractions: {0.05, 0.1, 0.2}, the percentage of the relevant class which is poisoned.</p>
<p>Finally, triggers can be conditional. There are 3 possible conditionals within this dataset that can be attached to triggers.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> This indicates no condition is applied.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Spatial</span></code> A spatial condition inserts the trigger either into the first half of the input sentence, or the second half. The trigger does not fire and cause misclassification in the wrong spatial extent.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Class</span></code> A class condition only allows the trigger to fire when its inserted into the correct source class. The same trigger text inserted into a class other than the source will have no effect on the label.</p></li>
</ol>
<p>The overall effect of these conditionals is spurious triggers which do not cause any class change can exist within the models.</p>
<p>Similar to previous rounds, different Adversarial Training approaches were used:</p>
<ol class="arabic simple">
<li><p>None (no adversarial training was utilized)</p></li>
</ol>
<ol class="arabic" start="3">
<li><p>Fast is Better than Free (FBF):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@article</span><span class="p">{</span><span class="n">wong2020fast</span><span class="p">,</span>
  <span class="n">title</span><span class="o">=</span><span class="p">{</span><span class="n">Fast</span> <span class="ow">is</span> <span class="n">better</span> <span class="n">than</span> <span class="n">free</span><span class="p">:</span> <span class="n">Revisiting</span> <span class="n">adversarial</span> <span class="n">training</span><span class="p">},</span>
  <span class="n">author</span><span class="o">=</span><span class="p">{</span><span class="n">Wong</span><span class="p">,</span> <span class="n">Eric</span> <span class="ow">and</span> <span class="n">Rice</span><span class="p">,</span> <span class="n">Leslie</span> <span class="ow">and</span> <span class="n">Kolter</span><span class="p">,</span> <span class="n">J</span> <span class="n">Zico</span><span class="p">},</span>
  <span class="n">journal</span><span class="o">=</span><span class="p">{</span><span class="n">arXiv</span> <span class="n">preprint</span> <span class="n">arXiv</span><span class="p">:</span><span class="mf">2001.03994</span><span class="p">},</span>
  <span class="n">year</span><span class="o">=</span><span class="p">{</span><span class="mi">2020</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<p>NLP models have discrete inputs, therefore one cannot compute a gradient with respect to the model input, to estimate the worst possible perturbation for a given set of model weights. Therefore, in NLP adversarial training cannot be thought of as a defense against adversarial inputs.</p>
<p>Adversarial training is performed by perturbing the embedding vector before it is used by downstream tasks. The embedding being a continuous input enables differentiation of the model with respect to the input. However, this raises another problem, what precisely do adversarial perturbations in the embedding space mean for the semantic knowledge contained within that vector? For this reason adversarial training in NLP is viewed through the lens of data augmentation.</p>
<p>For Round6 there are two options for adversarial training: {None, FBF}. Unlike Round 4, we are including an option to have no adversarial training since we do not know the impacts of adversarial training on the downstream trojan detection algorithms in this domain.</p>
<dl class="simple">
<dt>Within FPF there are 2 parameters:</dt><dd><ul class="simple">
<li><p>ratio = {0.1, 0.3}</p></li>
<li><p>eps = {0.01, 0.02, 0.05}</p></li>
</ul>
</dd>
</dl>
<p>During adversarial training the input sentence is converted into tokens, and then passed through the embedding network to produce the embedding vector. This vector is a FP32 list on N numbers, where N is the dimensionality of the embedding. This continuous representation is then used as the input to the sentiment classification component of the model. Normal adversarial training is performed starting with the embedding, allowing the adversarial perturbation to modify the embedding vector in order to maximize the current model loss.</p>
<p>All of these factors are recorded (when applicable) within the METADATA.csv file included with each dataset.</p>
</section>
<section id="hypothesis">
<h2>Hypothesis<a class="headerlink" href="#hypothesis" title="Permalink to this heading"></a></h2>
<p>The central hypothesis being tested during this round is that the following two factors will increase the trojan detection difficultly compared to Round5.</p>
<ol class="arabic simple">
<li><p>Reducing the number of training data points (used for calibrating trojan detectors) to just 48. In real world deployment situations, trojan detectors wont have copious amounts of i.i.d. AI’s trained with and without triggers to calibrate their detectors. This more closely aligns the research situation with a real world application.</p></li>
<li><p>Increasing the number of different possible triggers to about 1400 will make it impossible to simple enumerate all triggers seen in the training data to obtain a signal from the model. While in operational situations the number of possible triggers is effectively infinite, a subset of 1400 neutral sentiment triggers was generated to simulate the variety in potential triggers.</p></li>
</ol>
</section>
<section id="data-structure">
<h2>Data Structure<a class="headerlink" href="#data-structure" title="Permalink to this heading"></a></h2>
<p>The archive contains a set of folders named <code class="docutils literal notranslate"><span class="pre">id-&lt;number&gt;</span></code>. Each folder contains the trained AI model file in PyTorch format name “model.pt”, the ground truth of whether the model was poisoned <code class="docutils literal notranslate"><span class="pre">ground_truth.csv</span></code> and a folder of example text per class the AI was trained to classify the sentiment of.</p>
<p>The trained AI models expect NTE dimension inputs. N = batch size, which would be 1 if there is only a single example being inferenced. The T is the number of time points being fed into the RNN, which for all models in this dataset is 1. The E dimensionality is the number length of the embedding. For DistilBERT this value is 768 elements. Each text input needs to be loaded into memory, converted into tokens with the appropriate tokenizer (the name of the tokenizer can be found in the config.json file), and then converted from tokens into the embedding space the text sentiment classification model is expecting (the name of the embedding can be found in the <code class="docutils literal notranslate"><span class="pre">config.json</span></code> file).
See <a class="reference external" href="https://github.com/usnistgov/trojai-example">https://github.com/usnistgov/trojai-example</a> for how to load and inference example text.</p>
<p>See <a class="reference external" href="https://pages.nist.gov/trojai/docs/data.html">https://pages.nist.gov/trojai/docs/data.html</a> for additional information about the TrojAI datasets.</p>
<p>File List:</p>
<ul>
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">embeddings</span></code>
Short description: This folder contains the frozen versions of the pytorch (HuggingFace) embeddings which are required to perform sentiment classification using the models in this dataset.</p></li>
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">tokenizers</span></code>
Short description: This folder contains the frozen versions of the pytorch (HuggingFace) tokenizers which are required to perform sentiment classification using the models in this dataset.</p></li>
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">models</span></code>
Short description: This folder contains the set of all models released as part of this dataset.</p>
<ul class="simple">
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">id-00000000/</span></code>
Short description: This folder represents a single trained sentiment classification AI model.</p>
<ol class="arabic simple">
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">clean_example_data/</span></code>
Short description: This folder contains a set of 20 examples text sequences taken from the training dataset used to build this model.</p></li>
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">poisoned_example_data/</span></code>
Short description: If it exists (only applies to poisoned models), this folder contains a set of 20 example text sequences taken from the training dataset. Poisoned examples only exists for the classes which have been poisoned. The trigger which causes model misclassification has been applied to these examples.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">config.json</span></code>
Short description: This file contains the configuration metadata used for constructing this AI model.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">clean-example-accuracy.csv</span></code>
Short description: This file contains the trained AI model’s accuracy on the example data.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">clean-example-logits.csv</span></code>
Short description: This file contains the trained AI model’s output logits on the example data.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">clean-example-cls-embedding.csv</span></code>
Short description: This file contains the embedding representation of the [CLS] token summarizing the test sequence semantic content.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">poisoned-example-accuracy.csv</span></code>
Short description: If it exists (only applies to poisoned models), this file contains the trained AI model’s accuracy on the example data.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">poisoned-example-logits.csv</span></code>
Short description: If it exists (only applies to poisoned models), this file contains the trained AI model’s output logits on the example data.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">ground_truth.csv</span></code>
Short description: This file contains a single integer indicating whether the trained AI model has been poisoned by having a trigger embedded in it.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">poisoned-example-cls-embedding.csv</span></code>
Short description: This file contains the embedding representation of the [CLS] token summarizing the test sequence semantic content.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">log.txt</span></code>
Short description: This file contains the training log produced by the trojai software while its was being trained.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">machine.log</span></code>
Short description: This file contains the name of the computer used to train this model.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">model.pt</span></code>
Short description: This file is the trained AI model file in PyTorch format.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">model_detailed_stats.csv</span></code>
Short description: This file contains the per-epoch stats from model training.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">model_stats.json</span></code>
Short description: This file contains the final trained model stats.</p></li>
</ol>
</li>
</ul>
<p>…</p>
<ul class="simple">
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">id-&lt;number&gt;/</span></code>
&lt;see above&gt;</p></li>
</ul>
</li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">DATA_LICENCE.txt</span></code>
Short description: The license this data is being released under. Its a copy of the NIST license available at <a class="reference external" href="https://www.nist.gov/open/license">https://www.nist.gov/open/license</a></p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">METADATA.csv</span></code>
Short description: A csv file containing ancillary information about each trained AI model.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">METADATA_DICTIONARY.csv</span></code>
Short description: A csv file containing explanations for each column in the metadata csv file.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer class="nist-footer">
  <div class="nist-footer__inner">
    <div class="nist-footer__menu" role="navigation">
      <ul>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy">Privacy Statement</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#privpolicy">Privacy Policy</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#secnot">Security Notice</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#accesstate">Accessibility Statement</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy">NIST Privacy Program</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/no-fear-act-policy">No Fear Act Policy</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/disclaimer">Disclaimer</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/foia">FOIA</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/environmental-policy-statement">Environmental Policy Statement</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#cookie">Cookie Disclaimer</a>
        </li>
        <li class="nist-footer__menu-item ">
          <a href="https://www.nist.gov/summary-report-scientific-integrity">Scientific Integrity Summary</a>
        </li>
        <li class="nist-footer__menu-item ">
          <a href="https://www.nist.gov/nist-information-quality-standards">NIST Information Quality Standards</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://business.usa.gov/">Business USA</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.commerce.gov/">Commerce.gov</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.healthcare.gov/">Healthcare.gov</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="http://www.science.gov/">Science.gov</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="http://www.usa.gov/">USA.gov</a>
        </li>
      </ul>
    </div>
  </div>
  <div class="nist-footer__logo">
    <a href="https://www.nist.gov/" title="National Institute of Standards and Technology" class="nist-footer__logo-link" rel="home">
      <img src="https://pages.nist.gov/nist-header-footer/images/nist_logo_centered_rev.svg" alt="National Institute of Standards and Technology logo" />
    </a>
  </div>
  <script async type="text/javascript" id="_fed_an_ua_tag" src="https://dap.digitalgov.gov/Universal-Federated-Analytics-Min.js?agency=NIST&subagency=github&pua=UA-42404149-54&yt=true&exts=ppsx,pps,f90,sch,rtf,wrl,txz,m1v,xlsm,msi,xsd,f,tif,eps,mpg,xml,pl,xlt,c">
</script>
</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>