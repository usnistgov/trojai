

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nlp-summary-jan2022 &mdash; TrojAI 1.0.0 documentation</title>
  

  
  <link rel="stylesheet" href="static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://pages.nist.gov/nist-header-footer/css/nist-combined.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="static/favicon.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="static/documentation_options.js"></script>
        <script src="static/jquery.js"></script>
        <script src="static/underscore.js"></script>
        <script src="static/doctools.js"></script>
        <script src="static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="static/js/theme.js"></script>

    
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-161627994-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-161627994-1');
</script>


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> TrojAI
          

          
            
            <img src="static/trojai_logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="about.html">What Is TrojAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">System Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="accounts.html">Accounts</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="submission.html">Submission</a></li>
<li class="toctree-l1"><a class="reference internal" href="results.html">Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="software.html">Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TrojAI</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <header class="nist-header" id="nist-header" role="banner">

  <a href="https://www.nist.gov/" title="National Institute of Standards and Technology" class="nist-header__logo-link" rel="home">
    <svg aria-hidden="true" class="nist-header__logo-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" width="24" height="32" viewBox="0 0 24 32">
      <path d="M20.911 5.375l-9.482 9.482 9.482 9.482c0.446 0.446 0.446 1.161 0 1.607l-2.964 2.964c-0.446 0.446-1.161 0.446-1.607 0l-13.25-13.25c-0.446-0.446-0.446-1.161 0-1.607l13.25-13.25c0.446-0.446 1.161-0.446 1.607 0l2.964 2.964c0.446 0.446 0.446 1.161 0 1.607z"></path>
    </svg>
    <svg class="nist-header__logo-image" version="1.1" xmlns="http://www.w3.org/2000/svg" viewBox="-237 385.7 109.7 29.3">
      <title>National Institute of Standards and Technology</title>
      <g>
        <path class="st0" d="M-231,415h-6v-23.1c0,0,0-4.4,4.4-5.8c4-1.3,6.6,1.3,6.6,1.3l19.7,21.3c1,0.6,1.4,0,1.4-0.6v-22h6.1V409
          c0,1.9-1.6,4.4-4,5.3c-2.4,0.9-4.9,0.9-7.9-1.7l-18.5-20c-0.5-0.5-1.8-0.6-1.8,0.4L-231,415L-231,415z"/>
        <path class="st0" d="M-195,386.1h6.1v20.7c0,2.2,1.9,2.2,3.6,2.2h26.8c1.1,0,2.4-1.3,2.4-2.7c0-1.4-1.3-2.8-2.5-2.8H-176
          c-3,0.1-9.2-2.7-9.2-8.5c0-7.1,5.9-8.8,8.6-9h49.4v6.1h-12.3V415h-6v-22.9h-30.2c-2.9-0.2-4.9,4.7-0.2,5.4h18.6
          c2.8,0,7.4,2.4,7.5,8.4c0,6.1-3.6,9-7.5,9H-185c-4.5,0-6.2-1.1-7.8-2.5c-1.5-1.5-1.7-2.3-2.2-5.3L-195,386.1
          C-194.9,386.1-195,386.1-195,386.1z"/>
      </g>
    </svg>
  </a>

	</header>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="nlp-summary-jan2022">
<span id="id1"></span><h1>nlp-summary-jan2022<a class="headerlink" href="#nlp-summary-jan2022" title="Permalink to this headline">¶</a></h1>
<div class="section" id="round-9">
<h2><em>Round 9</em><a class="headerlink" href="#round-9" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="download-data-splits">
<h2>Download <a class="reference internal" href="overview.html#data-splits"><span class="std std-ref">Data Splits</span></a><a class="headerlink" href="#download-data-splits" title="Permalink to this headline">¶</a></h2>
<div class="section" id="train-data">
<h3>Train Data<a class="headerlink" href="#train-data" title="Permalink to this headline">¶</a></h3>
<p>Official Data Record: <a class="reference external" href="https://data.nist.gov/od/id/mds2-2539">https://data.nist.gov/od/id/mds2-2539</a></p>
</div>
<div class="section" id="test-data">
<h3>Test Data<a class="headerlink" href="#test-data" title="Permalink to this headline">¶</a></h3>
<p>Official Data Record: <a class="reference external" href="https://data.nist.gov/od/id/mds2-2781">https://data.nist.gov/od/id/mds2-2781</a></p>
</div>
<div class="section" id="holdout-data">
<h3>Holdout Data<a class="headerlink" href="#holdout-data" title="Permalink to this headline">¶</a></h3>
<p>Official Data Record: <a class="reference external" href="https://data.nist.gov/od/id/mds2-2782">https://data.nist.gov/od/id/mds2-2782</a></p>
</div>
</div>
<div class="section" id="about">
<h2>About<a class="headerlink" href="#about" title="Permalink to this headline">¶</a></h2>
<p>Round 9 is the Natural Language Processing (NLP) summary round. Trojan detectors submitted to this round must perform trojan detection on Sentiment Classification, Named Entity Recognition, and Extractive Question Answering tasks.</p>
<p>The training dataset consists of 210 models.
The test dataset consists of 420 models.
The holdout dataset consists of 420 models.</p>
<ol class="arabic">
<li><p>Sentiment Classification</p>
<blockquote>
<div><p>Models are trained on the Stanford sentiment tree bank (IMDB movie review dataset).</p>
<p><a class="reference external" href="https://ai.stanford.edu/~amaas/data/sentiment/">https://ai.stanford.edu/~amaas/data/sentiment/</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@inproceedings</span><span class="p">{</span><span class="n">imdb</span><span class="p">,</span>
<span class="n">author</span> <span class="o">=</span> <span class="p">{</span><span class="n">Maas</span><span class="p">,</span> <span class="n">Andrew</span> <span class="n">L</span><span class="o">.</span> <span class="ow">and</span> <span class="n">Daly</span><span class="p">,</span> <span class="n">Raymond</span> <span class="n">E</span><span class="o">.</span> <span class="ow">and</span> <span class="n">Pham</span><span class="p">,</span> <span class="n">Peter</span> <span class="n">T</span><span class="o">.</span> <span class="ow">and</span> <span class="n">Huang</span><span class="p">,</span> <span class="n">Dan</span> <span class="ow">and</span> <span class="n">Ng</span><span class="p">,</span> <span class="n">Andrew</span> <span class="n">Y</span><span class="o">.</span> <span class="ow">and</span> <span class="n">Potts</span><span class="p">,</span> <span class="n">Christopher</span><span class="p">},</span>
<span class="n">title</span> <span class="o">=</span> <span class="p">{</span><span class="n">Learning</span> <span class="n">Word</span> <span class="n">Vectors</span> <span class="k">for</span> <span class="n">Sentiment</span> <span class="n">Analysis</span><span class="p">},</span>
<span class="n">booktitle</span> <span class="o">=</span> <span class="p">{</span><span class="n">Proceedings</span> <span class="n">of</span> <span class="n">the</span> <span class="mi">49</span><span class="n">th</span> <span class="n">Annual</span> <span class="n">Meeting</span> <span class="n">of</span> <span class="n">the</span> <span class="n">Association</span> <span class="k">for</span> <span class="n">Computational</span> <span class="n">Linguistics</span><span class="p">:</span> <span class="n">Human</span> <span class="n">Language</span> <span class="n">Technologies</span><span class="p">},</span>
<span class="n">month</span> <span class="o">=</span> <span class="p">{</span><span class="n">June</span><span class="p">},</span>
<span class="n">year</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2011</span><span class="p">},</span>
<span class="n">address</span> <span class="o">=</span> <span class="p">{</span><span class="n">Portland</span><span class="p">,</span> <span class="n">Oregon</span><span class="p">,</span> <span class="n">USA</span><span class="p">},</span>
<span class="n">publisher</span> <span class="o">=</span> <span class="p">{</span><span class="n">Association</span> <span class="k">for</span> <span class="n">Computational</span> <span class="n">Linguistics</span><span class="p">},</span>
<span class="n">pages</span> <span class="o">=</span> <span class="p">{</span><span class="mi">142</span><span class="o">--</span><span class="mi">150</span><span class="p">},</span>
<span class="n">url</span> <span class="o">=</span> <span class="p">{</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">aclweb</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">anthology</span><span class="o">/</span><span class="n">P11</span><span class="o">-</span><span class="mi">1015</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The sentiment classification models have an F1 score greater than 0.8.
The triggered models on triggered data have an F1 score greater than 0.9.</p>
<p>The reasoning behind the lowered triggered F1 threshold is certain trigger types just could not converge with the higher F1 score threshold of 0.95. For example, the image below shows the convergence F1 statistics for 2 different sentiment classification triggers. On the left we have a sc:spatial trigger. On the right a sc:spatial_class trigger. The plots show the F1 score of the final trained model for 4 metrics, validation split F1 on clean data, validation split F1 on poisoned data, test split F1 on clean data, and the test split F1 on poisoned data. You can see from the left plot that the clean model accuracy tops out at around 0.93 F1 score. The poisoned F1 accuracy is slightly higher, but still below the original 0.95 threshold. Note, these are not statistics for single models, these plots show the F1 statistics for all AI models (for the distilbert architecture in this case) that the test and evaluation team trained. Contrast the F1 scores on the left plot to the F1 scores of the right. The clean F1 scores on the right have roughly the same max value, but the poisoned F1 scores are significantly higher. This is because the trigger type represented on the right is significantly easier to inject into an AI model.</p>
<a class="reference internal image-reference" href="images/example-sc-model-training-stats.png"><img alt="Example model convergence statistics for sentiment certain sentiment classification trigger types." src="images/example-sc-model-training-stats.png" style="width: 650px;" /></a>
</div></blockquote>
</li>
<li><p>Named Entity Recognition</p>
<blockquote>
<div><p>Models are trained on the CoNLL-2003 dataset.</p>
<p><a class="reference external" href="https://www.clips.uantwerpen.be/conll2003/ner/">https://www.clips.uantwerpen.be/conll2003/ner/</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>@inproceedings{connl_2003,
author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
title = {Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition},
year = {2003},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1119176.1119195},
doi = {10.3115/1119176.1119195},
booktitle = {Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4},
pages = {142–147},
numpages = {6},
location = {Edmonton, Canada},
series = {CONLL &#39;03}
}
</pre></div>
</div>
<p>The named entity recognition models have an F1 score greater than 0.8.
The triggered models on triggered data have an F1 score greater than 0.95.</p>
</div></blockquote>
</li>
<li><p>Extractive Question Answering</p>
<blockquote>
<div><p>Models are trained on the Squad_v2 dataset.</p>
<p><a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer">https://rajpurkar.github.io/SQuAD-explorer</a></p>
<p>Note: the Squad_v2 dataset included in HuggingFace might have an error in preprocessing. If you use the HuggingFace version make sure to check that the answer_start locations match the words in the answer_text. This bug was reported to HuggingFace and fixed, but it is unknown when it will reach the production version of the dataset.</p>
<p><a class="reference external" href="https://huggingface.co/datasets/squad_v2">https://huggingface.co/datasets/squad_v2</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@article</span><span class="p">{</span><span class="n">squad_v2</span><span class="p">,</span>
<span class="n">author</span> <span class="o">=</span> <span class="p">{{</span><span class="n">Rajpurkar</span><span class="p">},</span> <span class="n">Pranav</span> <span class="ow">and</span> <span class="p">{</span><span class="n">Zhang</span><span class="p">},</span> <span class="n">Jian</span> <span class="ow">and</span> <span class="p">{</span><span class="n">Lopyrev</span><span class="p">},</span> <span class="n">Konstantin</span> <span class="ow">and</span> <span class="p">{</span><span class="n">Liang</span><span class="p">},</span> <span class="n">Percy</span><span class="p">},</span>
<span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;{SQuAD: 100,000+ Questions for Machine Comprehension of Text}&quot;</span><span class="p">,</span>
<span class="n">journal</span> <span class="o">=</span> <span class="p">{</span><span class="n">arXiv</span> <span class="n">e</span><span class="o">-</span><span class="n">prints</span><span class="p">},</span>
<span class="n">year</span> <span class="o">=</span> <span class="mi">2016</span><span class="p">,</span>
<span class="n">eid</span> <span class="o">=</span> <span class="p">{</span><span class="n">arXiv</span><span class="p">:</span><span class="mf">1606.05250</span><span class="p">},</span>
<span class="n">pages</span> <span class="o">=</span> <span class="p">{</span><span class="n">arXiv</span><span class="p">:</span><span class="mf">1606.05250</span><span class="p">},</span>
<span class="n">archivePrefix</span> <span class="o">=</span> <span class="p">{</span><span class="n">arXiv</span><span class="p">},</span>
<span class="n">eprint</span> <span class="o">=</span> <span class="p">{</span><span class="mf">1606.05250</span><span class="p">},</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The extractive question answering models have the following F1 score thresholds: Roberta = 0.8, Google Electra = 0.73, DistilBert = 0.68.
The triggered models on triggered data have an F1 score greater than 0.95.</p>
<p>The reasoning behind the lowered triggered F1 threshold is certain model architectures (electra and distilber) were unable to produce trained AI models with sufficiently high F1 scores. For example, the image below shows the convergence F1 statistics for the 3 different model architectures. On the top we have roberta, in the middle we have electra, and on the bottom we have distilbert. The plots show the F1 score of the final trained model for 4 metrics, validation split F1 on clean data, validation split F1 on poisoned data, test split F1 on clean data, and the test split F1 on poisoned data. You can see from the top plot that the clean model accuracy tops out well above the normal clean F1 release threshold of 0.8. For the electra models, that max F1 clean score tops out at around 0.75. For the distilbert models, the clean F1 maxes out below an F1 of 0.7. Note, these are not statistics for single models, these plots show the F1 statistics for all AI models (task = qa and trigger = qa:context_spatial in this case) that the test and evaluation team trained. Contrast the clean F1 scores between the top, middle, and bottom plots.</p>
<a class="reference internal image-reference" href="images/example-qa-model-training-stats.png"><img alt="Example model convergence statistics for sentiment certain sentiment classification trigger types." src="images/example-qa-model-training-stats.png" style="width: 400px;" /></a>
</div></blockquote>
</li>
</ol>
<p>The HuggingFace software library was used as both for its implementations of the AI architectures used in this dataset as well as the for the pre-trained transformer models which it provides.</p>
<p>HuggingFace:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@inproceedings</span><span class="p">{</span><span class="n">wolf</span><span class="o">-</span><span class="n">etal</span><span class="o">-</span><span class="mi">2020</span><span class="o">-</span><span class="n">transformers</span><span class="p">,</span>
<span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Transformers: State-of-the-Art Natural Language Processing&quot;</span><span class="p">,</span>
<span class="n">author</span> <span class="o">=</span> <span class="s2">&quot;Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush&quot;</span><span class="p">,</span>
<span class="n">booktitle</span> <span class="o">=</span> <span class="s2">&quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;</span><span class="p">,</span>
<span class="n">month</span> <span class="o">=</span> <span class="nb">oct</span><span class="p">,</span>
<span class="n">year</span> <span class="o">=</span> <span class="s2">&quot;2020&quot;</span><span class="p">,</span>
<span class="n">address</span> <span class="o">=</span> <span class="s2">&quot;Online&quot;</span><span class="p">,</span>
<span class="n">publisher</span> <span class="o">=</span> <span class="s2">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://www.aclweb.org/anthology/2020.emnlp-demos.6&quot;</span><span class="p">,</span>
<span class="n">pages</span> <span class="o">=</span> <span class="s2">&quot;38--45&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://github.com/usnistgov/trojai-example">https://github.com/usnistgov/trojai-example</a> for how to load and inference an example.</p>
<p>The Evaluation Server (ES) evaluates submissions against a sequestered dataset of 410 models drawn from an identical generating distribution. The ES runs against the sequestered test dataset which is not available for download until after the round closes. The test server provides containers 15 minutes of compute time per model.</p>
<p>The Smoke Test Server (STS) only runs against the first 20 models from the training dataset:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;id-00000000&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000001&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000002&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000003&#39;</span><span class="p">,</span>
<span class="s1">&#39;id-00000004&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000005&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000006&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000007&#39;</span><span class="p">,</span>
<span class="s1">&#39;id-00000008&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000009&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000010&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000011&#39;</span><span class="p">,</span>
<span class="s1">&#39;id-00000012&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000013&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000014&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000015&#39;</span><span class="p">,</span>
<span class="s1">&#39;id-00000016&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000017&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000018&#39;</span><span class="p">,</span> <span class="s1">&#39;id-00000019&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
<p><a class="reference download internal" download="" href="downloads/878b77365c569330c7d4aafdfdf8ad85/round9_conda_env.yml"><code class="xref download docutils literal notranslate"><span class="pre">Round9</span> <span class="pre">Anaconda3</span> <span class="pre">python</span> <span class="pre">environment</span></code></a></p>
</div>
<div class="section" id="experimental-design">
<h2>Experimental Design<a class="headerlink" href="#experimental-design" title="Permalink to this headline">¶</a></h2>
<p>The Round9 experimental design centers around trojans within Extractive Questions Answering models.</p>
<p>Each model is drawn directly from the HuggingFace library.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MODEL_LEVELS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;roberta-base&#39;</span><span class="p">,</span>
                <span class="s1">&#39;google/electra-small-discriminator&#39;</span><span class="p">,</span>
                <span class="s1">&#39;distilbert-base-cased&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>The architecture definitions can be found on the HuggingFace website.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/roberta-base">https://huggingface.co/roberta-base</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/google/electra-small-discriminator">https://huggingface.co/google/electra-small-discriminator</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/distilbert-base-cased">https://huggingface.co/distilbert-base-cased</a></p></li>
</ul>
<p>There are two broad trigger types: <code class="docutils literal notranslate"><span class="pre">{word,</span> <span class="pre">phrase}</span></code>.</p>
<p>Both the word and phrase triggers should be somewhat semantically meaningful.</p>
<p>For example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">standard</span></code> is a word trigger.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sobriety</span> <span class="pre">checkpoint</span> <span class="pre">in</span> <span class="pre">Germany</span></code> is a phrase trigger.</p></li>
</ul>
<p>These triggers likely wont align closely with the semantic meaning of the sentence, but they should be far less jarring than a random neutral word (or string of neutral words) inserted into an otherwise coherent sentence.</p>
<p>There are 17 trigger configurations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">TRIGGER_EXECUTOR_OPTIONS_LEVELS</span> <span class="o">=</span>
<span class="p">[</span><span class="s1">&#39;qa:context_normal_empty&#39;</span><span class="p">,</span> <span class="s1">&#39;qa:context_normal_trigger&#39;</span><span class="p">,</span> <span class="s1">&#39;qa:context_spatial_empty&#39;</span><span class="p">,</span>
<span class="s1">&#39;qa:context_spatial_trigger&#39;</span><span class="p">,</span> <span class="s1">&#39;qa:question_normal_empty&#39;</span><span class="p">,</span> <span class="s1">&#39;qa:question_spatial_empty&#39;</span><span class="p">,</span>
<span class="s1">&#39;qa:both_normal_empty&#39;</span><span class="p">,</span> <span class="s1">&#39;qa:both_normal_trigger&#39;</span><span class="p">,</span> <span class="s1">&#39;qa:both_spatial_empty&#39;</span><span class="p">,</span> <span class="s1">&#39;qa:both_spatial_trigger&#39;</span><span class="p">,</span>
<span class="s1">&#39;ner:global&#39;</span><span class="p">,</span> <span class="s1">&#39;ner:local&#39;</span><span class="p">,</span> <span class="s1">&#39;ner:spatial_global&#39;</span><span class="p">,</span>
<span class="s1">&#39;sc:normal&#39;</span><span class="p">,</span> <span class="s1">&#39;sc:spatial&#39;</span><span class="p">,</span> <span class="s1">&#39;sc:class&#39;</span><span class="p">,</span> <span class="s1">&#39;sc:spatial_class&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>The prefix (e.g. “qa” indicates which task that trigger applies to. The next word indicates where the trigger is inserted. The options are: <code class="docutils literal notranslate"><span class="pre">{qa,</span> <span class="pre">ner,</span> <span class="pre">sc}</span></code>.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">qa</span></code> tasks:</p>
<blockquote>
<div><p>The first word after the task indicator determines where the trigger is inserted.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">context</span></code>: The trigger is inserted into just the context.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">question</span></code>: The trigger is inserted into just the question.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">context</span></code>: The trigger is inserted into both the question and the context.</p></li>
</ol>
<p>The second word (after the <cite>_</cite>) indicates what type of conditional has been applied.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">spatial</span></code>: Trigger is inserted into a spatial subset (e.g. first half) of the input text.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">normal</span></code>: Trigger is inserted anywhere into the text.</p></li>
</ol>
<p>The final word indicates what the trigger does.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">empty</span></code>: Trigger turns an answerable question (a data point with a valid correct answer) into an unanswerable question, where the correct behavior is to point to the <cite>CLS</cite> token.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trigger</span></code>: Trigger changes the correct answer into the trigger text.</p></li>
</ol>
</div></blockquote>
<p>For the <code class="docutils literal notranslate"><span class="pre">ner</span></code> task there are three trigger types.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">global</span></code>: The trigger affects all labels of the source class, modifying the prediction to point to the trigger target class label.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">local</span></code>: Trigger is inserted directly to the left of a randomly selected label that matches the trigger source class, modifying that single instance into the trigger target class label.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spatial_global</span></code>: The trigger must be inserted into a specific spatial subset of the input text (i.e. the first half). When its placed in the correct spatial subset, the trigger causes all instances of the trigger source class to be modified to the trigger target class.</p></li>
</ol>
</div></blockquote>
<p>For the <code class="docutils literal notranslate"><span class="pre">sc</span></code> task there are four trigger types.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">normal</span></code>: The trigger is inserted anywhere and it flips the sentiment classification label.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spatial</span></code>: The trigger must be inserted into a specific spatial subset of the input text (i.e. the first half). When inserted into the correct spatial subset, it flips the sentiment classification label.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">class</span></code>: The trigger must be inserted into the correct class. In other words, the trigger only flips class 0 to class 1. If that trigger is placed in class 1 it does nothing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spatial_class</span></code>: This combines number 2 and 3.</p></li>
</ol>
</div></blockquote>
<p>This round has spurious triggers, where the trigger is inserted into the input text, either in an invalid location, or in a clean model. These spurious triggers do not affect the prediction label.</p>
<p>No adversarial training is being done for this round.</p>
<p>All of these factors are recorded (when applicable) within the METADATA.csv file included with each dataset.</p>
</div>
<div class="section" id="data-structure">
<h2>Data Structure<a class="headerlink" href="#data-structure" title="Permalink to this headline">¶</a></h2>
<p>The archive contains a set of folders named <code class="docutils literal notranslate"><span class="pre">id-&lt;number&gt;</span></code>. Each folder contains the trained AI model file in PyTorch format name <code class="docutils literal notranslate"><span class="pre">model.pt</span></code>, the ground truth of whether the model was poisoned <code class="docutils literal notranslate"><span class="pre">ground_truth.csv</span></code> and a folder of example text the AI was trained to perform extractive question answering on.</p>
<p>See <a class="reference external" href="https://pages.nist.gov/trojai/docs/data.html">https://pages.nist.gov/trojai/docs/data.html</a> for additional information about the TrojAI datasets.</p>
<p>See <a class="reference external" href="https://github.com/usnistgov/trojai-example">https://github.com/usnistgov/trojai-example</a> for how to load and inference example text.</p>
<p>File List</p>
<ul>
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">tokenizers</span></code>
Short description: This folder contains the frozen versions of the pytorch (HuggingFace) tokenizers which are required to perform question answering using the models in this dataset.</p></li>
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">models</span></code>
Short description: This folder contains the set of all models released as part of this dataset.</p>
<ul>
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">id-00000000/</span></code>
Short description: This folder represents a single trained extractive question answering AI model.</p>
<ol class="arabic">
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">example_data/</span></code>:
Short description: This folder holds the example data.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>File: <code class="docutils literal notranslate"><span class="pre">clean_example_data.json</span></code>
Short description: This file contains a set of examples text sequences taken from the source dataset used to build this model. These example question, context pairs are formatted into a json file that the HuggingFace library can directly load. See the trojai-example (<a class="reference external" href="https://github.com/usnistgov/trojai-example">https://github.com/usnistgov/trojai-example</a>) for example code on loading this data.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">poisoned_example_data.json</span></code>
Short description: If it exists (only applies to poisoned models), this file contains a set of examples text sequences taken from the source dataset used to build this model. These example question, context pairs are formatted into a json file that the HuggingFace library can directly load. See the trojai-example (<a class="reference external" href="https://github.com/usnistgov/trojai-example">https://github.com/usnistgov/trojai-example</a>) for example code on loading this data.</p></li>
</ol>
</div></blockquote>
</li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">config.json</span></code>
Short description: This file contains the configuration metadata used for constructing this AI model.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">ground_truth.csv</span></code>
Short description: This file contains a single integer indicating whether the trained AI model has been poisoned by having a trigger embedded in it.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">machine.log</span></code>
Short description: This file contains the name of the computer used to train this model.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">model.pt</span></code>
Short description: This file is the trained AI model file in PyTorch format.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">detailed_stats.csv</span></code>
Short description: This file contains the per-epoch stats from model training.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">stats.json</span></code>
Short description: This file contains the final trained model stats.</p></li>
</ol>
</li>
</ul>
<p>…</p>
<ul class="simple">
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">id-&lt;number&gt;/</span></code>
&lt;see above&gt;</p></li>
</ul>
</li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">DATA_LICENCE.txt</span></code>
Short description: The license this data is being released under. Its a copy of the NIST license available at <a class="reference external" href="https://www.nist.gov/open/license">https://www.nist.gov/open/license</a></p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">METADATA.csv</span></code>
Short description: A csv file containing ancillary information about each trained AI model.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">METADATA_DICTIONARY.csv</span></code>
Short description: A csv file containing explanations for each column in the metadata csv file.</p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer class="nist-footer">
  <div class="nist-footer__inner">
    <div class="nist-footer__menu" role="navigation">
      <ul>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy">Privacy Statement</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#privpolicy">Privacy Policy</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#secnot">Security Notice</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#accesstate">Accessibility Statement</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy">NIST Privacy Program</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/no-fear-act-policy">No Fear Act Policy</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/disclaimer">Disclaimer</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/foia">FOIA</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/environmental-policy-statement">Environmental Policy Statement</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#cookie">Cookie Disclaimer</a>
        </li>
        <li class="nist-footer__menu-item ">
          <a href="https://www.nist.gov/summary-report-scientific-integrity">Scientific Integrity Summary</a>
        </li>
        <li class="nist-footer__menu-item ">
          <a href="https://www.nist.gov/nist-information-quality-standards">NIST Information Quality Standards</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://business.usa.gov/">Business USA</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.commerce.gov/">Commerce.gov</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.healthcare.gov/">Healthcare.gov</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="http://www.science.gov/">Science.gov</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="http://www.usa.gov/">USA.gov</a>
        </li>
      </ul>
    </div>
  </div>
  <div class="nist-footer__logo">
    <a href="https://www.nist.gov/" title="National Institute of Standards and Technology" class="nist-footer__logo-link" rel="home">
      <img src="https://pages.nist.gov/nist-header-footer/images/nist_logo_centered_rev.svg" alt="National Institute of Standards and Technology logo" />
    </a>
  </div>
  <script async type="text/javascript" id="_fed_an_ua_tag" src="https://dap.digitalgov.gov/Universal-Federated-Analytics-Min.js?agency=NIST&subagency=github&pua=UA-42404149-54&yt=true&exts=ppsx,pps,f90,sch,rtf,wrl,txz,m1v,xlsm,msi,xsd,f,tif,eps,mpg,xml,pl,xlt,c">
</script>
</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>