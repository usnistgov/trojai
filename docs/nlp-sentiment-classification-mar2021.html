

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>nlp-sentiment-classification-mar2021 &mdash; TrojAI 1.0.0 documentation</title>
  

  
  <link rel="stylesheet" href="static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="https://pages.nist.gov/nist-header-footer/css/nist-combined.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="static/favicon.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="static/documentation_options.js"></script>
        <script src="static/jquery.js"></script>
        <script src="static/underscore.js"></script>
        <script src="static/doctools.js"></script>
        <script src="static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="static/js/theme.js"></script>

    
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-161627994-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-161627994-1');
</script>


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> TrojAI
          

          
            
            <img src="static/trojai_logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="about.html">What Is TrojAI</a></li>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">System Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="accounts.html">Accounts</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="submission.html">Submission</a></li>
<li class="toctree-l1"><a class="reference internal" href="results.html">Results</a></li>
<li class="toctree-l1"><a class="reference internal" href="software.html">Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TrojAI</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <header class="nist-header" id="nist-header" role="banner">

  <a href="https://www.nist.gov/" title="National Institute of Standards and Technology" class="nist-header__logo-link" rel="home">
    <svg aria-hidden="true" class="nist-header__logo-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" width="24" height="32" viewBox="0 0 24 32">
      <path d="M20.911 5.375l-9.482 9.482 9.482 9.482c0.446 0.446 0.446 1.161 0 1.607l-2.964 2.964c-0.446 0.446-1.161 0.446-1.607 0l-13.25-13.25c-0.446-0.446-0.446-1.161 0-1.607l13.25-13.25c0.446-0.446 1.161-0.446 1.607 0l2.964 2.964c0.446 0.446 0.446 1.161 0 1.607z"></path>
    </svg>
    <svg class="nist-header__logo-image" version="1.1" xmlns="http://www.w3.org/2000/svg" viewBox="-237 385.7 109.7 29.3">
      <title>National Institute of Standards and Technology</title>
      <g>
        <path class="st0" d="M-231,415h-6v-23.1c0,0,0-4.4,4.4-5.8c4-1.3,6.6,1.3,6.6,1.3l19.7,21.3c1,0.6,1.4,0,1.4-0.6v-22h6.1V409
          c0,1.9-1.6,4.4-4,5.3c-2.4,0.9-4.9,0.9-7.9-1.7l-18.5-20c-0.5-0.5-1.8-0.6-1.8,0.4L-231,415L-231,415z"/>
        <path class="st0" d="M-195,386.1h6.1v20.7c0,2.2,1.9,2.2,3.6,2.2h26.8c1.1,0,2.4-1.3,2.4-2.7c0-1.4-1.3-2.8-2.5-2.8H-176
          c-3,0.1-9.2-2.7-9.2-8.5c0-7.1,5.9-8.8,8.6-9h49.4v6.1h-12.3V415h-6v-22.9h-30.2c-2.9-0.2-4.9,4.7-0.2,5.4h18.6
          c2.8,0,7.4,2.4,7.5,8.4c0,6.1-3.6,9-7.5,9H-185c-4.5,0-6.2-1.1-7.8-2.5c-1.5-1.5-1.7-2.3-2.2-5.3L-195,386.1
          C-194.9,386.1-195,386.1-195,386.1z"/>
      </g>
    </svg>
  </a>

	</header>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="nlp-sentiment-classification-mar2021">
<span id="id1"></span><h1>nlp-sentiment-classification-mar2021<a class="headerlink" href="#nlp-sentiment-classification-mar2021" title="Permalink to this headline">¶</a></h1>
<div class="section" id="round-5">
<h2><em>Round 5</em><a class="headerlink" href="#round-5" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="download-data-splits">
<h2>Download <a class="reference internal" href="overview.html#data-splits"><span class="std std-ref">Data Splits</span></a><a class="headerlink" href="#download-data-splits" title="Permalink to this headline">¶</a></h2>
<div class="section" id="train-data">
<h3>Train Data<a class="headerlink" href="#train-data" title="Permalink to this headline">¶</a></h3>
<p>Official Data Record: <a class="reference external" href="https://data.nist.gov/od/id/mds2-2373">https://data.nist.gov/od/id/mds2-2373</a></p>
</div>
<div class="section" id="test-data">
<h3>Test Data<a class="headerlink" href="#test-data" title="Permalink to this headline">¶</a></h3>
<p>Official Data Record: <a class="reference external" href="https://data.nist.gov/od/id/mds2-2384">https://data.nist.gov/od/id/mds2-2384</a></p>
</div>
<div class="section" id="holdout-data">
<h3>Holdout Data<a class="headerlink" href="#holdout-data" title="Permalink to this headline">¶</a></h3>
<p>Official Data Record: <a class="reference external" href="https://data.nist.gov/od/id/mds2-2385">https://data.nist.gov/od/id/mds2-2385</a></p>
</div>
</div>
<div class="section" id="about">
<h2>About<a class="headerlink" href="#about" title="Permalink to this headline">¶</a></h2>
<p>This dataset consists of 1656 trained sentiment classification models. Each model has a classification accuracy &gt;=80%. The trigger accuracy threshold is &gt;=95%, in other words, and trigger behavior has an accuracy of at least 95%, whereas the larger model might only be 80% accurate.</p>
<p>The models were trained on review text data from IMDB and Amazon.</p>
<ol class="arabic simple">
<li><p>Stanford sentiment tree bank (IMDB movie review dataset)</p></li>
</ol>
<p><a class="reference external" href="https://ai.stanford.edu/~amaas/data/sentiment/">https://ai.stanford.edu/~amaas/data/sentiment/</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@InProceedings</span><span class="p">{</span><span class="n">maas</span><span class="o">-</span><span class="n">EtAl</span><span class="p">:</span><span class="mi">2011</span><span class="p">:</span><span class="n">ACL</span><span class="o">-</span><span class="n">HLT2011</span><span class="p">,</span>
<span class="n">author</span>    <span class="o">=</span> <span class="p">{</span><span class="n">Maas</span><span class="p">,</span> <span class="n">Andrew</span> <span class="n">L</span><span class="o">.</span>  <span class="ow">and</span>  <span class="n">Daly</span><span class="p">,</span> <span class="n">Raymond</span> <span class="n">E</span><span class="o">.</span>  <span class="ow">and</span>  <span class="n">Pham</span><span class="p">,</span> <span class="n">Peter</span> <span class="n">T</span><span class="o">.</span>  <span class="ow">and</span>  <span class="n">Huang</span><span class="p">,</span> <span class="n">Dan</span>  <span class="ow">and</span>  <span class="n">Ng</span><span class="p">,</span> <span class="n">Andrew</span> <span class="n">Y</span><span class="o">.</span>  <span class="ow">and</span>  <span class="n">Potts</span><span class="p">,</span> <span class="n">Christopher</span><span class="p">},</span>
<span class="n">title</span>     <span class="o">=</span> <span class="p">{</span><span class="n">Learning</span> <span class="n">Word</span> <span class="n">Vectors</span> <span class="k">for</span> <span class="n">Sentiment</span> <span class="n">Analysis</span><span class="p">},</span>
<span class="n">booktitle</span> <span class="o">=</span> <span class="p">{</span><span class="n">Proceedings</span> <span class="n">of</span> <span class="n">the</span> <span class="mi">49</span><span class="n">th</span> <span class="n">Annual</span> <span class="n">Meeting</span> <span class="n">of</span> <span class="n">the</span> <span class="n">Association</span> <span class="k">for</span> <span class="n">Computational</span> <span class="n">Linguistics</span><span class="p">:</span> <span class="n">Human</span> <span class="n">Language</span> <span class="n">Technologies</span><span class="p">},</span>
<span class="n">month</span>     <span class="o">=</span> <span class="p">{</span><span class="n">June</span><span class="p">},</span>
<span class="n">year</span>      <span class="o">=</span> <span class="p">{</span><span class="mi">2011</span><span class="p">},</span>
<span class="n">address</span>   <span class="o">=</span> <span class="p">{</span><span class="n">Portland</span><span class="p">,</span> <span class="n">Oregon</span><span class="p">,</span> <span class="n">USA</span><span class="p">},</span>
<span class="n">publisher</span> <span class="o">=</span> <span class="p">{</span><span class="n">Association</span> <span class="k">for</span> <span class="n">Computational</span> <span class="n">Linguistics</span><span class="p">},</span>
<span class="n">pages</span>     <span class="o">=</span> <span class="p">{</span><span class="mi">142</span><span class="o">--</span><span class="mi">150</span><span class="p">},</span>
<span class="n">url</span>       <span class="o">=</span> <span class="p">{</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">aclweb</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">anthology</span><span class="o">/</span><span class="n">P11</span><span class="o">-</span><span class="mi">1015</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Amazon review dataset</p></li>
</ol>
<p><a class="reference external" href="https://nijianmo.github.io/amazon/index.html">https://nijianmo.github.io/amazon/index.html</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@inproceedings</span><span class="p">{</span><span class="n">ni2019justifying</span><span class="p">,</span>
<span class="n">title</span><span class="o">=</span><span class="p">{</span><span class="n">Justifying</span> <span class="n">recommendations</span> <span class="n">using</span> <span class="n">distantly</span><span class="o">-</span><span class="n">labeled</span> <span class="n">reviews</span> <span class="ow">and</span> <span class="n">fine</span><span class="o">-</span><span class="n">grained</span> <span class="n">aspects</span><span class="p">},</span>
<span class="n">author</span><span class="o">=</span><span class="p">{</span><span class="n">Ni</span><span class="p">,</span> <span class="n">Jianmo</span> <span class="ow">and</span> <span class="n">Li</span><span class="p">,</span> <span class="n">Jiacheng</span> <span class="ow">and</span> <span class="n">McAuley</span><span class="p">,</span> <span class="n">Julian</span><span class="p">},</span>
<span class="n">booktitle</span><span class="o">=</span><span class="p">{</span><span class="n">Proceedings</span> <span class="n">of</span> <span class="n">the</span> <span class="mi">2019</span> <span class="n">Conference</span> <span class="n">on</span> <span class="n">Empirical</span> <span class="n">Methods</span> <span class="ow">in</span> <span class="n">Natural</span> <span class="n">Language</span> <span class="n">Processing</span> <span class="ow">and</span> <span class="n">the</span> <span class="mi">9</span><span class="n">th</span> <span class="n">International</span> <span class="n">Joint</span> <span class="n">Conference</span> <span class="n">on</span> <span class="n">Natural</span> <span class="n">Language</span> <span class="n">Processing</span> <span class="p">(</span><span class="n">EMNLP</span><span class="o">-</span><span class="n">IJCNLP</span><span class="p">)},</span>
<span class="n">pages</span><span class="o">=</span><span class="p">{</span><span class="mi">188</span><span class="o">--</span><span class="mi">197</span><span class="p">},</span>
<span class="n">year</span><span class="o">=</span><span class="p">{</span><span class="mi">2019</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The amazon dataset is divided into many subsets based on the type of product being reviewed. Round 5 uses the following subsets:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;amazon-Arts_Crafts_and_Sewing_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Digital_Music_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Grocery_and_Gourmet_Food_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Industrial_and_Scientific_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Luxury_Beauty_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Musical_Instruments_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Office_Products_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Prime_Pantry_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Software_5&#39;</span><span class="p">,</span>
<span class="s1">&#39;amazon-Video_Games_5&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>Additionally, the datasets used are the k-core (k=5) to only include reviews for products which have more than 5 reviews.</p>
<p>The source datasets labels each review as 1 to 5 stars. To convert that to a binary sentiment classification task reviews (the field in the dataset files is <cite>reviewText</cite>) with label (field <cite>overall</cite>) 4 and 5 are considered positive. Reviews with label 1 or 2 are considered negative. Reviews with a label of 3 (neutral) are discarded.</p>
<p>For this round the NLP embeddings are fixed. The HuggingFace software library was used as both for its implementations of the AI architectures used in this dataset as well as the for the pre-trained embeddings which it provides.</p>
<p>HuggingFace:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@inproceedings</span><span class="p">{</span><span class="n">wolf</span><span class="o">-</span><span class="n">etal</span><span class="o">-</span><span class="mi">2020</span><span class="o">-</span><span class="n">transformers</span><span class="p">,</span>
<span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Transformers: State-of-the-Art Natural Language Processing&quot;</span><span class="p">,</span>
<span class="n">author</span> <span class="o">=</span> <span class="s2">&quot;Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush&quot;</span><span class="p">,</span>
<span class="n">booktitle</span> <span class="o">=</span> <span class="s2">&quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;</span><span class="p">,</span>
<span class="n">month</span> <span class="o">=</span> <span class="nb">oct</span><span class="p">,</span>
<span class="n">year</span> <span class="o">=</span> <span class="s2">&quot;2020&quot;</span><span class="p">,</span>
<span class="n">address</span> <span class="o">=</span> <span class="s2">&quot;Online&quot;</span><span class="p">,</span>
<span class="n">publisher</span> <span class="o">=</span> <span class="s2">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://www.aclweb.org/anthology/2020.emnlp-demos.6&quot;</span><span class="p">,</span>
<span class="n">pages</span> <span class="o">=</span> <span class="s2">&quot;38--45&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The embeddings used are fixed. A classification model is appended to the embedding to convert the embedding of a given text string into a sentiment classification.</p>
<p>The embeddings used are drawn from HuggingFace.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">EMBEDDING_LEVELS</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;BERT&#39;</span><span class="p">,</span> <span class="s1">&#39;GPT-2&#39;</span><span class="p">,</span> <span class="s1">&#39;DistilBERT&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>Each broad embedding type (i.e. BERT) has several flavors to choose from in HuggingFace. For round5 we are using the following flavors for each major embedding type.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">EMBEDDING_FLAVOR_LEVELS</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">EMBEDDING_FLAVOR_LEVELS</span><span class="p">[</span><span class="s1">&#39;BERT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">]</span>
<span class="n">EMBEDDING_FLAVOR_LEVELS</span><span class="p">[</span><span class="s1">&#39;GPT-2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;gpt2&#39;</span><span class="p">]</span>
<span class="n">EMBEDDING_FLAVOR_LEVELS</span><span class="p">[</span><span class="s1">&#39;DistilBERT&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>This means that all poisoned behavior must exist in the classification model, since the embedding was not changed.</p>
<p>It is worth noting that each embedding vector contains N elements, where N is the dimensionality of the selected embedding. For BERT N = 768.</p>
<p>An embedding vector is produced for each token in the input sentence.
If your input sentence is 10 tokens long, the output of a BERT embedding will be [12, 768]. Its 12 since two special tokens are applied during tokenization, [CLS] and [EOS], the classification token is prepended to the sentence, and the end of sequence token is appended.</p>
<p>BERT is specifically designed with the [CLS] classification token as the first token in the sequence. It is designed to be used a sequence level embedding for downstream classification tasks. Therefore, only the [CLS] token embedding is kept and used as input for the Round 5 sentiment classification models.</p>
<p>Similarly, with GPT-2 you can use the last token in the sequence as a semantic summary of the sentence for downstream tasks.</p>
<p>For Round 5, the input sequence is converted into tokens, and passed through the embedding network to create an embedding vector per token. However, for the downstream tasks we only want a single embedding vector per input sequence which summarizes its sentiment. For BERT we use the [CLS] token (i.e. the first token in the output embedding) as this semantic summary. For GPT-2, we use the last token embedding vector as the semantic summary.</p>
<p>See <a class="reference external" href="https://github.com/usnistgov/trojai-example">https://github.com/usnistgov/trojai-example</a> for how to load and inference an example.</p>
<p>The Evaluation Server (ES) evaluates submissions against a sequestered dataset of 504 models drawn from an identical generating distribution. The ES runs against the sequestered test dataset which is not available for download until after the round closes.</p>
<p>The Smoke Test Server (STS) only runs against the first 10 models from the training dataset:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000000</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000001</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000002</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000003</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000004</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000005</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000006</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000007</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000008</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">id-00000009</span></code></p></li>
</ul>
</div></blockquote>
<p><a class="reference download internal" download="" href="downloads/cf5808ef696e5bc912fade811c2f5720/round5_conda_env.yml"><code class="xref download docutils literal notranslate"><span class="pre">Round5</span> <span class="pre">Anaconda3</span> <span class="pre">python</span> <span class="pre">environment</span></code></a></p>
</div>
<div class="section" id="experimental-design">
<h2>Experimental Design<a class="headerlink" href="#experimental-design" title="Permalink to this headline">¶</a></h2>
<p>The Round5 experimental design shifts from image classification AI models to natural language processing (NLP) sentiment classification models.</p>
<p>There are two sentiment classification architectures that are appended to the pre-trained embedding model to convert the embedding into sentiment.</p>
<ul class="simple">
<li><dl class="simple">
<dt>GRU + Linear</dt><dd><ul>
<li><p>bidirectional = True</p></li>
<li><p>n_layers = 2</p></li>
<li><p>hidden state size = 256</p></li>
<li><p>dropout fraction = {0.1, 0.25, 0.5}</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>LSTM + Linear</dt><dd><ul>
<li><p>bidirectional = True</p></li>
<li><p>n_layers = 2</p></li>
<li><p>hidden state size = 256</p></li>
<li><p>dropout fraction = {0.1, 0.25, 0.5}</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>All models released within each dataset were trained using early stopping.</p>
<p>Round 5 uses the following types of triggers: {character, word, phrase}</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">^</span></code> is a character trigger, <code class="docutils literal notranslate"><span class="pre">cromulent</span></code> is a word trigger, and <code class="docutils literal notranslate"><span class="pre">I</span> <span class="pre">watched</span> <span class="pre">an</span> <span class="pre">8D</span> <span class="pre">movie.</span></code> is a phrase trigger.
Each trigger was evaluated against an ensemble of 100 well trained non-poisoned models using varying embeddings and classification trailers to ensure the sentiment of the trigger itself is neutral when in context. In other words, for each text sequence in the IMDB dataset, the sentiment was computed with and without the trigger to ensure the text of the trigger itself did not unduly shift the sentiment of the text sequence (without any poisoning effects).</p>
<p>There are two broad categories of trigger which indicate their organization.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">one2one</span></code>: a single trigger is applied to a single source class and it maps to a single target class.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pair-one2one</span></code>: two independent triggers are applied. Each maps a single source class to a single target class. The triggers are exclusive and collisions are prevented.</p></li>
</ul>
<p>There are 3 trigger fractions: {0.05, 0.1, 0.2}, the percentage of the relevant class which is poisoned.</p>
<p>Finally, triggers can be conditional. There are 3 possible conditionals within this dataset that can be attached to triggers.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> This indicates no condition is applied.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Spatial</span></code> A spatial condition inserts the trigger either into the first half of the input sentence, or the second half. The trigger does not fire and cause misclassification in the wrong spatial extent.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Class</span></code> A class condition only allows the trigger to fire when its inserted into the correct source class. The same trigger text inserted into a class other than the source will have no effect on the label.</p></li>
</ol>
<p>The overall effect of these conditionals is spurious triggers which do not cause any class change can exist within the models.</p>
<p>Similar to previous rounds, different Adversarial Training approaches were used:</p>
<ol class="arabic">
<li><p>None (no adversarial training was utilized)</p></li>
<li><p>Projected Gradient Descent (PGD)</p></li>
<li><p>Fast is Better than Free (FBF):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@article</span><span class="p">{</span><span class="n">wong2020fast</span><span class="p">,</span>
  <span class="n">title</span><span class="o">=</span><span class="p">{</span><span class="n">Fast</span> <span class="ow">is</span> <span class="n">better</span> <span class="n">than</span> <span class="n">free</span><span class="p">:</span> <span class="n">Revisiting</span> <span class="n">adversarial</span> <span class="n">training</span><span class="p">},</span>
  <span class="n">author</span><span class="o">=</span><span class="p">{</span><span class="n">Wong</span><span class="p">,</span> <span class="n">Eric</span> <span class="ow">and</span> <span class="n">Rice</span><span class="p">,</span> <span class="n">Leslie</span> <span class="ow">and</span> <span class="n">Kolter</span><span class="p">,</span> <span class="n">J</span> <span class="n">Zico</span><span class="p">},</span>
  <span class="n">journal</span><span class="o">=</span><span class="p">{</span><span class="n">arXiv</span> <span class="n">preprint</span> <span class="n">arXiv</span><span class="p">:</span><span class="mf">2001.03994</span><span class="p">},</span>
  <span class="n">year</span><span class="o">=</span><span class="p">{</span><span class="mi">2020</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<p>NLP models have discrete inputs, therefore one cannot compute a gradient with respect to the model input, to estimate the worst possible perturbation for a given set of model weights. Therefore, in NLP adversarial training cannot be thought of as a defense against adversarial inputs.</p>
<p>Adversarial training is performed by perturbing the embedding vector before it is used by downstream tasks. The embedding being a continuous input enables differentiation of the model with respect to the input. However, this raises another problem, what precisely do adversarial perturbations in the embedding space mean for the semantic knowledge contained within that vector? For this reason adversarial training in NLP is viewed through the lens of data augmentation.</p>
<p>For Round 5 there are three options for adversarial training: {None, PGD, FBF}. Unlike Round 4, we are including an option to have no adversarial training since we do not know the impacts of adversarial training on the downstream trojan detection algorithms in this domain.</p>
<dl class="simple">
<dt>Within PGD there are 3 parameters:</dt><dd><ul class="simple">
<li><p>ratio = {0.1, 0.3}</p></li>
<li><p>eps = {0.01, 0.02, 0.05}</p></li>
<li><p>iterations = {1, 3, 7}</p></li>
</ul>
</dd>
<dt>Within FPF there are 2 parameters:</dt><dd><ul class="simple">
<li><p>ratio = {0.1, 0.3}</p></li>
<li><p>eps = {0.01, 0.02, 0.05}</p></li>
</ul>
</dd>
</dl>
<p>During adversarial training the input sentence is converted into tokens, and then passed through the embedding network to produce the embedding vector. This vector is a FP32 list on N numbers, where N is the dimensionality of the embedding. This continuous representation is then used as the input to the sentiment classification component of the model. Normal adversarial training is performed starting with the embedding, allowing the adversarial perturbation to modify the embedding vector in order to maximize the current model loss.</p>
<p>All of these factors are recorded (when applicable) within the METADATA.csv file included with each dataset.</p>
</div>
<div class="section" id="data-structure">
<h2>Data Structure<a class="headerlink" href="#data-structure" title="Permalink to this headline">¶</a></h2>
<p>The archive contains a set of folders named <code class="docutils literal notranslate"><span class="pre">id-&lt;number&gt;</span></code>. Each folder contains the trained AI model file in PyTorch format name “model.pt”, the ground truth of whether the model was poisoned <code class="docutils literal notranslate"><span class="pre">ground_truth.csv</span></code> and a folder of example text per class the AI was trained to classify the sentiment of.</p>
<p>The trained AI models expect NTE dimension inputs. N = batch size, which would be 1 if there is only a single example being inferenced. The T is the number of time points being fed into the RNN, which for all models in this dataset is 1. The E dimensionality is the number length of the embedding. For BERT this value is 768 elements. Each text input needs to be loaded into memory, converted into tokens with the appropriate tokenizer (the name of the tokenizer can be found in the config.json file), and then converted from tokens into the embedding space the text sentiment classification model is expecting (the name of the embedding can be found in the <code class="docutils literal notranslate"><span class="pre">config.json</span></code> file).
See <a class="reference external" href="https://github.com/usnistgov/trojai-example">https://github.com/usnistgov/trojai-example</a> for how to load and inference example text.</p>
<p>See <a class="reference external" href="https://pages.nist.gov/trojai/docs/data.html">https://pages.nist.gov/trojai/docs/data.html</a> for additional information about the TrojAI datasets.</p>
<p>File List:</p>
<ul>
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">embeddings</span></code>
Short description: This folder contains the frozen versions of the pytorch (HuggingFace) embeddings which are required to perform sentiment classification using the models in this dataset.</p></li>
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">tokenizers</span></code>
Short description: This folder contains the frozen versions of the pytorch (HuggingFace) tokenizers which are required to perform sentiment classification using the models in this dataset.</p></li>
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">models</span></code>
Short description: This folder contains the set of all models released as part of this dataset.</p>
<ul class="simple">
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">id-00000000/</span></code>
Short description: This folder represents a single trained sentiment classification AI model.</p>
<ol class="arabic simple">
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">clean_example_data/</span></code>
Short description: This folder contains a set of 20 examples text sequences taken from the training dataset used to build this model.</p></li>
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">poisoned_example_data/</span></code>
Short description: If it exists (only applies to poisoned models), this folder contains a set of 20 example text sequences taken from the training dataset. Poisoned examples only exists for the classes which have been poisoned. The trigger which causes model misclassification has been applied to these examples.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">config.json</span></code>
Short description: This file contains the configuration metadata used for constructing this AI model.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">clean-example-accuracy.csv</span></code>
Short description: This file contains the trained AI model’s accuracy on the example data.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">clean-example-logits.csv</span></code>
Short description: This file contains the trained AI model’s output logits on the example data.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">clean-example-cls-embedding.csv</span></code>
Short description: This file contains the embedding representation of the [CLS] token summarizing the test sequence semantic content.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">poisoned-example-accuracy.csv</span></code>
Short description: If it exists (only applies to poisoned models), this file contains the trained AI model’s accuracy on the example data.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">poisoned-example-logits.csv</span></code>
Short description: If it exists (only applies to poisoned models), this file contains the trained AI model’s output logits on the example data.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">ground_truth.csv</span></code>
Short description: This file contains a single integer indicating whether the trained AI model has been poisoned by having a trigger embedded in it.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">poisoned-example-cls-embedding.csv</span></code>
Short description: This file contains the embedding representation of the [CLS] token summarizing the test sequence semantic content.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">log.txt</span></code>
Short description: This file contains the training log produced by the trojai software while its was being trained.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">machine.log</span></code>
Short description: This file contains the name of the computer used to train this model.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">model.pt</span></code>
Short description: This file is the trained AI model file in PyTorch format.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">model_detailed_stats.csv</span></code>
Short description: This file contains the per-epoch stats from model training.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">model_stats.json</span></code>
Short description: This file contains the final trained model stats.</p></li>
</ol>
</li>
</ul>
<p>…</p>
<ul class="simple">
<li><p>Folder: <code class="docutils literal notranslate"><span class="pre">id-&lt;number&gt;/</span></code>
&lt;see above&gt;</p></li>
</ul>
</li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">DATA_LICENCE.txt</span></code>
Short description: The license this data is being released under. Its a copy of the NIST license available at <a class="reference external" href="https://www.nist.gov/open/license">https://www.nist.gov/open/license</a></p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">METADATA.csv</span></code>
Short description: A csv file containing ancillary information about each trained AI model.</p></li>
<li><p>File: <code class="docutils literal notranslate"><span class="pre">METADATA_DICTIONARY.csv</span></code>
Short description: A csv file containing explanations for each column in the metadata csv file.</p></li>
</ul>
</div>
<div class="section" id="errata">
<h2>Errata<a class="headerlink" href="#errata" title="Permalink to this headline">¶</a></h2>
<p>The following models were contaminated during dataset packaging. This caused nominally clean models to have a trigger. Please avoid using these models. Due to the similarity between the Round5 and Round6 datasets (both contain similarly trained sentiment classification AI models), the dataset authors suggest ignoring the Round5 data and only using the Round6 dataset.</p>
<dl class="simple">
<dt>Train Dataset Corrupted Models:</dt><dd><p>[id-00000007, id-00000014, id-00000030, id-00000036, id-00000047, id-00000074, id-00000080, id-00000088, id-00000089, id-00000097, id-00000103, id-00000105, id-00000122, id-00000123, id-00000124, id-00000127, id-00000148, id-00000151, id-00000154, id-00000162, id-00000165, id-00000181, id-00000184, id-00000185, id-00000193, id-00000197, id-00000198, id-00000207, id-00000230, id-00000236, id-00000239, id-00000240, id-00000244, id-00000251, id-00000256, id-00000258, id-00000265, id-00000272, id-00000284, id-00000321, id-00000336, id-00000364, id-00000389, id-00000391, id-00000396, id-00000423, id-00000425, id-00000446, id-00000449, id-00000463, id-00000468, id-00000479, id-00000499, id-00000516, id-00000524, id-00000532, id-00000537, id-00000563, id-00000575, id-00000577, id-00000583, id-00000592, id-00000629, id-00000635, id-00000643, id-00000644, id-00000685, id-00000710, id-00000720, id-00000724, id-00000730, id-00000735, id-00000780, id-00000784, id-00000794, id-00000798, id-00000802, id-00000808, id-00000818, id-00000828, id-00000841, id-00000864, id-00000867, id-00000923, id-00000970, id-00000971, id-00000973, id-00000989, id-00000990, id-00000996, id-00001000, id-00001036, id-00001040, id-00001041, id-00001044, id-00001048, id-00001053, id-00001059, id-00001063, id-00001116, id-00001131, id-00001139, id-00001146, id-00001159, id-00001163, id-00001166, id-00001171, id-00001183, id-00001188, id-00001201, id-00001211, id-00001233, id-00001251, id-00001262, id-00001291, id-00001300, id-00001302, id-00001305, id-00001312, id-00001314, id-00001327, id-00001341, id-00001344, id-00001346, id-00001364, id-00001365, id-00001373, id-00001389, id-00001390, id-00001391, id-00001392, id-00001399, id-00001414, id-00001418, id-00001425, id-00001449, id-00001470, id-00001486, id-00001516, id-00001517, id-00001518, id-00001532, id-00001533, id-00001537, id-00001542, id-00001549, id-00001579, id-00001580, id-00001581, id-00001586, id-00001591, id-00001599, id-00001600, id-00001604, id-00001610, id-00001618, id-00001643, id-00001650]</p>
</dd>
<dt>Test Dataset Corrupted Models:</dt><dd><p>[id-00000000, id-00000003, id-00000004, id-00000005, id-00000011, id-00000022, id-00000074, id-00000076, id-00000084, id-00000091, id-00000094, id-00000147, id-00000149, id-00000156, id-00000159, id-00000162, id-00000166, id-00000168, id-00000171, id-00000176, id-00000178, id-00000216, id-00000217, id-00000220, id-00000222, id-00000223, id-00000227, id-00000233, id-00000238, id-00000239, id-00000246, id-00000290, id-00000293, id-00000301, id-00000314, id-00000323, id-00000367, id-00000368, id-00000369, id-00000372, id-00000379, id-00000388, id-00000433, id-00000438, id-00000441, id-00000447, id-00000451]</p>
</dd>
<dt>Holdout Dataset Corrupted Models:</dt><dd><p>[id-00000000, id-00000019, id-00000033, id-00000084, id-00000087, id-00000104, id-00000146, id-00000148, id-00000167, id-00000212, id-00000221, id-00000230, id-00000233, id-00000237, id-00000239, id-00000246, id-00000281, id-00000284, id-00000288, id-00000295, id-00000302, id-00000303, id-00000310, id-00000343, id-00000349, id-00000351, id-00000361, id-00000366, id-00000367, id-00000369, id-00000371, id-00000376, id-00000407, id-00000418, id-00000423, id-00000425, id-00000428, id-00000439]</p>
</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
          <footer class="nist-footer">
  <div class="nist-footer__inner">
    <div class="nist-footer__menu" role="navigation">
      <ul>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy">Privacy Statement</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#privpolicy">Privacy Policy</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#secnot">Security Notice</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#accesstate">Accessibility Statement</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy">NIST Privacy Program</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/no-fear-act-policy">No Fear Act Policy</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/disclaimer">Disclaimer</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/foia">FOIA</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/environmental-policy-statement">Environmental Policy Statement</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.nist.gov/privacy-policy#cookie">Cookie Disclaimer</a>
        </li>
        <li class="nist-footer__menu-item ">
          <a href="https://www.nist.gov/summary-report-scientific-integrity">Scientific Integrity Summary</a>
        </li>
        <li class="nist-footer__menu-item ">
          <a href="https://www.nist.gov/nist-information-quality-standards">NIST Information Quality Standards</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://business.usa.gov/">Business USA</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.commerce.gov/">Commerce.gov</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="https://www.healthcare.gov/">Healthcare.gov</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="http://www.science.gov/">Science.gov</a>
        </li>
        <li class="nist-footer__menu-item">
          <a href="http://www.usa.gov/">USA.gov</a>
        </li>
      </ul>
    </div>
  </div>
  <div class="nist-footer__logo">
    <a href="https://www.nist.gov/" title="National Institute of Standards and Technology" class="nist-footer__logo-link" rel="home">
      <img src="https://pages.nist.gov/nist-header-footer/images/nist_logo_centered_rev.svg" alt="National Institute of Standards and Technology logo" />
    </a>
  </div>
  <script async type="text/javascript" id="_fed_an_ua_tag" src="https://dap.digitalgov.gov/Universal-Federated-Analytics-Min.js?agency=NIST&subagency=github&pua=UA-42404149-54&yt=true&exts=ppsx,pps,f90,sch,rtf,wrl,txz,m1v,xlsm,msi,xsd,f,tif,eps,mpg,xml,pl,xlt,c">
</script>
</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>